{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1- What is a parameter?\n",
        "Ans- A parameter is a configuration variable that is internal to the model and whose value is estimated from the data during training.\n",
        "Key Characteristics of Parameters:\n",
        "Learned: They are automatically adjusted by the algorithm as it learns from training data.\n",
        "Define the model: They are what the model uses to make predictions.\n",
        "\n",
        "Examples:\n",
        "In linear regression, the parameters are the weights (coefficients) and bias.\n",
        "In a neural network, the parameters are the weights and biases of all the neurons.\n",
        "In logistic regression, the parameters determine the shape and orientation of the decision boundary.\n",
        "\n",
        "Parameter vs Hyperparameter:\n",
        "Parameter: Learned from data.\n",
        "\n",
        "Hyperparameter: Set before training, controls the training process (e.g., learning rate, number of layers, batch size).\n",
        "\n",
        "Q2- What is correlation?\n",
        "Ans- Correlation refers to the statistical relationship between two variables — often used during data analysis and feature selection.\n",
        "\n",
        "Why Correlation Matters in Machine Learning:\n",
        "Helps identify relationships between features and the target variable.\n",
        "Helps detect redundant features (e.g. two features with high correlation may provide the same information).\n",
        "Useful in feature engineering and dimensionality reduction.\n",
        "\n",
        "Types of Correlation in ML:\n",
        "Feature-Feature Correlation\n",
        "Measures how features relate to each other.\n",
        "High correlation might indicate multicollinearity, which can hurt models like linear regression.\n",
        "Feature-Target Correlation\n",
        "Measures how a feature relates to the target variable.\n",
        "Helps identify predictive power of features.\n",
        "\n",
        "Common Correlation Metrics:\n",
        "Pearson correlation (linear): Measures linear relationship between numeric variables.\n",
        "Spearman rank correlation: Measures monotonic relationships (not necessarily linear).\n",
        "Point-Biserial correlation: Used when one variable is binary and the other is continuous.\n",
        "\n",
        "Example:\n",
        "In a dataset predicting house prices:\n",
        "House Size (sqft) and Price → likely a positive correlation\n",
        "Distance from city center and Price → likely a negative correlation\n",
        "\n",
        "- a negative correlation means that as one variable increases, the other tends to decrease, and vice versa.\n",
        "\n",
        "Practical Use:\n",
        "In exploratory data analysis (EDA) to understand relationships.\n",
        "In feature selection, you might remove one of two highly correlated features to avoid redundancy.\n",
        "\n",
        "Q3- Define Machine Learning. What are the main components in Machine Learning?\n",
        "Ans- Machine Learning (ML) is a subset of artificial intelligence (AI) that focuses on creating systems that can learn from data and make predictions or decisions without being explicitly programmed for each specific task.\n",
        "\n",
        "In simple terms:\n",
        "Machine learning enables computers to learn patterns from data and improve performance over time on a given task.\n",
        "\n",
        "Q4- How does loss value help in determining whether the model is good or not?\n",
        "Ans- The loss value is critical in machine learning because it tells you how well your model is performing — or more specifically, how far off its predictions are from the actual values.\n",
        "\n",
        "What Is a Loss Value?\n",
        "The loss is a single number that represents the error in your model's prediction on a specific input or batch of data.\n",
        "\n",
        "A smaller loss means the model's predictions are closer to the actual values — and the model is likely performing better.\n",
        "\n",
        "How Loss Helps Evaluate a Model:\n",
        "1. Guides Learning\n",
        "During training, the model uses the loss value to update its internal parameters (weights).\n",
        "The goal is to minimize the loss — that's how learning happens.\n",
        "\n",
        "2. Monitors Model Quality\n",
        "High loss → poor predictions\n",
        "Low loss → better predictions\n",
        "\n",
        "If the loss stops decreasing or starts increasing, it may indicate:\n",
        "The model is stuck (plateau)\n",
        "Overfitting (if loss on training goes down but validation goes up)\n",
        "\n",
        "3. Helps Compare Models\n",
        "You can compare the loss values of different models (or the same model with different settings) to decide which one performs best.\n",
        "\n",
        "Important: Loss ≠ Accuracy (always)\n",
        "Loss measures the magnitude of errors (more detailed).\n",
        "\n",
        "Accuracy simply measures how often predictions are right (only works well for classification tasks).\n",
        "\n",
        "For example:\n",
        "A model predicting 0.51 instead of 1 (true value) is technically accurate if you're rounding, but the loss captures how close or far it actually is.\n",
        "\n",
        "Q5- What are continuous and categorical variables?\n",
        "Ans- Continuous and categorical variables is essential in both machine learning and data analysis.\n",
        "\n",
        "1. Continuous Variables:\n",
        "These are numeric variables that can take any value within a range, including decimals.\n",
        "\n",
        "Characteristics:\n",
        "Can be measured.\n",
        "Have an infinite number of possible values.\n",
        "Support mathematical operations (e.g., mean, standard deviation).\n",
        "\n",
        "Examples:\n",
        "Height (e.g., 170.2 cm)\n",
        "Temperature (e.g., 36.5°C)\n",
        "Income (e.g., $42,750.99)\n",
        "Age (e.g., 23.8 years)\n",
        "\n",
        "2. Categorical Variables:\n",
        "These are variables that represent categories or groups, often as labels or names.\n",
        "\n",
        "Characteristics:\n",
        "Values are distinct categories, not numbers you can meaningfully add or average.\n",
        "Can be nominal (no order) or ordinal (have order).\n",
        "\n",
        "Examples:\n",
        "Nominal:\n",
        "Color (red, blue, green)\n",
        "Gender (male, female)\n",
        "Country (USA, India, Brazil)\n",
        "\n",
        "Ordinal:\n",
        "Education level (high school, bachelor’s, master’s, PhD)\n",
        "Customer satisfaction (low, medium, high)\n",
        "\n",
        "Why It Matters in Machine Learning:\n",
        "Continuous variables: Can go directly into models like linear regression or neural networks.\n",
        "Categorical variables: Often need to be encoded (e.g., one-hot encoding or label encoding) to be used in ML algorithms.\n",
        "\n",
        "Q6-How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "Ans- Categorical variables correctly is crucial in machine learning because most ML algorithms work only with numerical data. So, categorical values must be converted into a numerical format before feeding them into a model.\n",
        "\n",
        "Common Techniques to Handle Categorical Variables:\n",
        "1. Label Encoding\n",
        "Converts each category into a unique integer.\n",
        "Suitable for ordinal data (where order matters).\n",
        "\n",
        "| Size   | Encoded |\n",
        "| ------ | ------- |\n",
        "| Small  | 0       |\n",
        "| Medium | 1       |\n",
        "| Large  | 2       |\n",
        "\n",
        "Caution:\n",
        "Can mislead the model for nominal data (no order), as the numbers may imply a ranking.\n",
        "\n",
        "2. One-Hot Encoding\n",
        "Creates a new binary column for each category (1 if present, 0 if not).\n",
        "Best for nominal data.\n",
        "\n",
        "Example:\n",
        "| Color | Red | Blue | Green |\n",
        "| ----- | --- | ---- | ----- |\n",
        "| Red   | 1   | 0    | 0     |\n",
        "| Blue  | 0   | 1    | 0     |\n",
        "\n",
        "\n",
        "Used in:\n",
        "Tree-based models, linear models, neural networks, etc.\n",
        "\n",
        "3. Ordinal Encoding\n",
        "Similar to label encoding, but manual assignment of numeric values based on order.\n",
        "You define the relationship.\n",
        "| Education   | Ordinal |\n",
        "| ----------- | ------- |\n",
        "| High School | 0       |\n",
        "| Bachelor’s  | 1       |\n",
        "| Master’s    | 2       |\n",
        "| PhD         | 3       |\n",
        "\n",
        "4. Target Encoding (Mean Encoding)\n",
        "Replace categories with the mean of the target variable for each category.\n",
        "Useful for high-cardinality categorical features.\n",
        "Risk:\n",
        "May lead to overfitting if not handled properly (e.g., without cross-validation or smoothing).\n",
        "\n",
        "5. Binary Encoding / Hashing (Advanced)\n",
        "Binary Encoding: Converts categories to binary code and stores in fewer columns.\n",
        "Hashing: Uses a hash function to map categories to a fixed number of columns.\n",
        "Used when there are many unique categories (high cardinality).\n",
        "\n",
        "Choosing the Right Technique:\n",
        "| Scenario                           | Best Technique              |\n",
        "| ---------------------------------- | --------------------------- |\n",
        "| Ordinal categories (e.g., Size)    | Label or Ordinal Encoding   |\n",
        "| Nominal categories with few values | One-Hot Encoding            |\n",
        "| Many unique categories             | Target, Binary, or Hashing  |\n",
        "| Risk of overfitting                | One-Hot with Regularization |\n",
        "\n",
        "Q7- What do you mean by training and testing a dataset?\n",
        "Ans- In machine learning, the concepts of training and testing datasets are fundamental to building and evaluating models effectively.\n",
        "\n",
        "What Is Training a Dataset?\n",
        "The training dataset is the portion of your data used to teach the machine learning model.\n",
        "During training, the model learns patterns and adjusts its internal parameters (like weights in a neural network) based on this data.\n",
        "\n",
        "Example:\n",
        "If you're building a model to predict house prices:\n",
        "The training data includes features like square footage, number of rooms, location, etc.\n",
        "And the actual prices (labels), so the model learns the relationships.\n",
        "\n",
        "What Is Testing a Dataset?\n",
        "The testing dataset is a separate portion of the data that is not shown to the model during training.\n",
        "It's used to evaluate how well the trained model performs on new, unseen data.\n",
        "This simulates how the model will behave in the real world.\n",
        "\n",
        "Why Split into Train and Test Sets?\n",
        "To check generalization: Does the model work well only on training data, or also on unseen data?\n",
        "To detect overfitting: A model that memorizes the training data may perform poorly on the test data.\n",
        "\n",
        "Common Splits:\n",
        "80/20 split: 80% for training, 20% for testing\n",
        "70/30 or 90/10 depending on data size\n",
        "Sometimes a third set called a validation set is used to tune hyperparameters\n",
        "\n",
        "Summary:\n",
        "| Dataset  | Used For               | Model Sees It During Training? |\n",
        "| -------- | ---------------------- | ------------------------------ |\n",
        "| Training | Learning patterns      | ✅ Yes                          |\n",
        "| Testing  | Evaluating performance | ❌ No                           |\n",
        "\n",
        "Q8 - What is sklearn.preprocessing?\n",
        "Ans- sklearn.preprocessing is a module in the scikit-learn library that provides a variety of tools to prepare or transform your data before feeding it into a machine learning model.\n",
        "\n",
        "Preprocessing is essential because raw data often needs to be cleaned, normalized, encoded, or scaled to improve model performance.\n",
        "\n",
        "Why Use sklearn.preprocessing?\n",
        "Machine learning models often assume:\n",
        "All features are numeric\n",
        "Features are on similar scales\n",
        "Missing or categorical data is handled properly\n",
        "This module helps with all that.\n",
        "\n",
        "Q9- What is a Test set?\n",
        "Ans- A test set is a subset of your dataset that you set aside and don’t use during the training of your machine learning model. Its primary purpose is to evaluate the performance of the trained model on new, unseen data.\n",
        "\n",
        "Key points about a Test Set:\n",
        "Purpose: To provide an unbiased estimate of how well the model generalizes to data it hasn’t seen before.\n",
        "Separate from training data: The model does not learn from the test set — it only predicts on it.\n",
        "Helps detect overfitting: If a model performs well on training data but poorly on the test set, it might be overfitting (memorizing rather than learning patterns).\n",
        "Usually 10-30% of the full dataset, depending on data size.\n",
        "\n",
        "Summary:\n",
        "| Dataset      | Used For                     | Model Access During Training? |\n",
        "| ------------ | ---------------------------- | ----------------------------- |\n",
        "| Training set | Learning patterns            | Yes                           |\n",
        "| Test set     | Evaluating model performance | No                            |\n",
        "\n",
        "Q10- How do we split data for model fitting (training and testing) in Python?How do you approach a Machine Learning problem?\n",
        "Ans - Splitting data into training and testing sets is super important to build and evaluate a machine learning model properly.\n",
        "\n",
        "In Python, the easiest and most common way to do this is using train_test_split from scikit-learn.\n",
        "\n",
        "Here’s how to do it:\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Suppose X = features, y = target labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,           # Your data\n",
        "    test_size=0.2,  # 20% data for testing, 80% for training\n",
        "    random_state=42 # Fixes the randomness for reproducibility\n",
        ")\n",
        "\n",
        "Explanation:\n",
        "X: Your input features (can be a DataFrame or numpy array)\n",
        "y: Your target variable\n",
        "test_size=0.2: 20% of data goes to the test set, the rest (80%) for training\n",
        "random_state: Sets a seed to ensure you get the same split every time you run the code (important for reproducibility)\n",
        "\n",
        "Optional parameters:\n",
        "train_size: Specify how much data you want for training instead of test_size.\n",
        "shuffle: Default is True — shuffles data before splitting (usually a good idea).\n",
        "stratify: To maintain the same proportion of classes in train and test (important for classification).\n",
        "\n",
        "Example with stratification:\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.25,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "Q11 - Why do we have to perform EDA before fitting a model to the data?\n",
        "Ans- Exploratory Data Analysis (EDA) before fitting a model is critical in machine learning. EDA helps you understand your dataset, uncover data issues, and make informed decisions about preprocessing, feature selection, and modeling strategies.\n",
        "\n",
        "\n",
        "Why Perform EDA Before Model Fitting?\n",
        "1. Understand the Structure and Distribution of Data\n",
        "Identify data types (categorical vs continuous)\n",
        "View distributions of features (e.g., normal, skewed, bimodal)\n",
        "Detect outliers, zero values, and class imbalances\n",
        "Example: A target variable with 90% of one class may require resampling.\n",
        "\n",
        "2. Detect Missing or Corrupt Data\n",
        "EDA helps find missing values or invalid entries that could break your model.\n",
        "Lets you decide how to handle them: remove, fill, or flag.\n",
        "Example: If 40% of values in a column are missing, you might drop it or use imputation.\n",
        "\n",
        "3. Reveal Relationships Between Variables\n",
        "Correlations between features and the target (and among features themselves).\n",
        "Helps with feature selection and reducing redundancy (multicollinearity).\n",
        "Example: Two highly correlated features? Keep only one.\n",
        "\n",
        "4. Identify Potential Data Leaks\n",
        "A feature that too closely relates to the target might cause data leakage, giving you unrealistically high accuracy.\n",
        "Example: A \"total price\" column in a model predicting \"price per item\".\n",
        "\n",
        "5. Choose the Right Preprocessing Methods\n",
        "Decide which features need encoding, scaling, or transformation.\n",
        "Example:\n",
        "Categorical → One-hot encode\n",
        "Skewed numerical → Apply log transformation\n",
        "\n",
        "6. Guide Model Selection\n",
        "The nature of the data (linear vs nonlinear, class imbalance, etc.) can influence which models or techniques you should try.\n",
        "\n",
        "Common EDA Techniques:\n",
        "Summary stats (df.describe(), .info())\n",
        "\n",
        "Visualizations:\n",
        "Histograms\n",
        "Box plots\n",
        "Correlation heatmaps\n",
        "Scatter plots\n",
        "Missing value checks\n",
        "Groupby analysis (e.g., mean target per category)\n",
        "\n",
        "In Summary:\n",
        "EDA is like reading the instruction manual before using a complex machine.\n",
        "It helps you avoid costly mistakes and build better, more accurate models.\n",
        "\n",
        "Q12- What is correlation?\n",
        "Ans - Correlation refers to the statistical relationship between two variables — often used during data analysis and feature selection.\n",
        "\n",
        "Why Correlation Matters in Machine Learning:\n",
        "Helps identify relationships between features and the target variable.\n",
        "Helps detect redundant features (e.g. two features with high correlation may provide the same information).\n",
        "Useful in feature engineering and dimensionality reduction.\n",
        "\n",
        "Types of Correlation in ML:\n",
        "Feature-Feature Correlation\n",
        "Measures how features relate to each other.\n",
        "High correlation might indicate multicollinearity, which can hurt models like linear regression.\n",
        "Feature-Target Correlation\n",
        "Measures how a feature relates to the target variable.\n",
        "Helps identify predictive power of features.\n",
        "\n",
        "Common Correlation Metrics:\n",
        "Pearson correlation (linear): Measures linear relationship between numeric variables.\n",
        "Spearman rank correlation: Measures monotonic relationships (not necessarily linear).\n",
        "Point-Biserial correlation: Used when one variable is binary and the other is continuous.\n",
        "\n",
        "Example:\n",
        "In a dataset predicting house prices:\n",
        "House Size (sqft) and Price → likely a positive correlation\n",
        "Distance from city center and Price → likely a negative correlation\n",
        "\n",
        "- a negative correlation means that as one variable increases, the other tends to decrease, and vice versa.\n",
        "\n",
        "Practical Use:\n",
        "In exploratory data analysis (EDA) to understand relationships.\n",
        "In feature selection, you might remove one of two highly correlated features to avoid redundancy.\n",
        "\n",
        "Q13- What does negative correlation mean?\n",
        "Ans - Negative correlation means that as one variable increases, the other decreases, and vice versa.\n",
        "\n",
        "In other words:\n",
        "When X goes up, Y tends to go down\n",
        "When X goes down, Y tends to go up\n",
        "\n",
        "Correlation Coefficient (r):\n",
        "Ranges from –1 to 1\n",
        "r = –1: Perfect negative correlation\n",
        "r = 0: No correlation\n",
        "r = –0.7: Strong negative correlation\n",
        "r = –0.3: Weak negative correlation\n",
        "\n",
        "Example:\n",
        "| Hours of TV Watched | Exam Score |\n",
        "| ------------------- | ---------- |\n",
        "| 5                   | 60         |\n",
        "| 4                   | 65         |\n",
        "| 3                   | 75         |\n",
        "| 2                   | 85         |\n",
        "| 1                   | 95         |\n",
        "\n",
        "As TV time increases, exam scores decrease → strong negative correlation.\n",
        "\n",
        "Why It's Important in ML:\n",
        "Helps identify inverse relationships between features.\n",
        "If two features are strongly negatively correlated, one might be removed to reduce redundancy.\n",
        "Helps in feature selection, data understanding, and model interpretation.\n",
        "\n",
        "Visual Clue:\n",
        "In a scatter plot, negative correlation appears as a downward slope from left to right.\n",
        "\n",
        "Q14- How can you find correlation between variables in Python?\n",
        "Ans- You can find correlation between variables in Python using pandas and visualization libraries like seaborn or matplotlib.\n",
        "\n",
        "1. Using pandas.corr()\n",
        "This method computes the Pearson correlation coefficient by default (which measures linear relationships).\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Example DataFrame\n",
        "data = {\n",
        "    'Hours_Studied': [1, 2, 3, 4, 5],\n",
        "    'Exam_Score': [55, 60, 65, 70, 75]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "print(correlation_matrix)\n",
        "\n",
        "2. Visualizing with a Heatmap (seaborn)\n",
        "This is useful when you have many variables.\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create the heatmap\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()\n",
        "\n",
        "3. For Spearman or Kendall Correlation\n",
        "Use .corr(method='spearman') or .corr(method='kendall') if:\n",
        "The data is not linear\n",
        "You're working with ranked or ordinal data\n",
        "\n",
        "df.corr(method='spearman')\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "| Correlation Value | Interpretation    |\n",
        "| ----------------- | ----------------- |\n",
        "| 1.0               | Perfect positive  |\n",
        "| 0.7 to 1.0        | Strong positive   |\n",
        "| 0.3 to 0.7        | Moderate positive |\n",
        "| 0.0 to 0.3        | Weak positive     |\n",
        "| 0                 | No correlation    |\n",
        "| –0.3 to 0         | Weak negative     |\n",
        "| –0.7 to –0.3      | Moderate negative |\n",
        "| –1.0 to –0.7      | Strong negative   |\n",
        "| –1.0              | Perfect negative  |\n",
        "\n",
        "Q15- What is causation? Explain difference between correlation and causation with an example.\n",
        "Ans- Causation means that one variable directly affects another — a cause-and-effect relationship.\n",
        "If X causes Y, then changes in X will produce changes in Y.\n",
        "\n",
        "In contrast, correlation only means that two variables move together, but it does not imply that one causes the other.\n",
        "\n",
        "Example:\n",
        "Scenario:\n",
        "A study finds that:\n",
        "\n",
        "Ice cream sales and drowning incidents have a strong positive correlation.\n",
        "\n",
        "Correlation:\n",
        "As ice cream sales increase, drownings also increase.\n",
        "But does buying ice cream cause drowning? No.\n",
        "\n",
        "Causation:\n",
        "The real cause behind both is hot weather (a third variable or confounder).\n",
        "Hot weather → more people buy ice cream\n",
        "Hot weather → more people swim → more drowning accidents\n",
        "So there's a correlation, but no causation between ice cream and drowning.\n",
        "\n",
        "In Machine Learning:\n",
        "Correlation can help select features, but beware:\n",
        "Causation is much harder to prove and often requires:\n",
        "\n",
        "Experiments (A/B testing)\n",
        "Domain knowledge\n",
        "Causal inference methods\n",
        "\n",
        "Summary:\n",
        "Correlation is a signal; causation is a fact.\n",
        "Just because two things move together doesn’t mean one causes the other.\n",
        "\n",
        "\n",
        "Q16- What is an Optimizer? What are different types of optimizers? Explain each\n",
        "with an example.\n",
        "Ans- An optimizer is an algorithm used during model training to adjust the model's parameters (like weights and biases) in order to minimize the loss function and improve predictions.\n",
        "\n",
        "In simpler terms:\n",
        "Optimizers help the model learn by telling it how to change its parameters to get better results.\n",
        "\n",
        "Why Are Optimizers Important?\n",
        "The goal of training is to minimize the loss (error). Optimizers decide how the model should update its parameters after each step, based on how much the current prediction differs from the actual value.\n",
        "\n",
        "Most Common Types of Optimizers:\n",
        "1. Gradient Descent (GD)\n",
        "Idea:\n",
        "Uses the gradient (slope) of the loss function to move the parameters in the direction that reduces the loss.\n",
        "Limitation:\n",
        "Computes gradients using entire dataset — slow and memory-heavy for large data.\n",
        "Example:# Used more conceptually than directly in practice\n",
        "\n",
        "\n",
        "2. Stochastic Gradient Descent (SGD)\n",
        "Idea:\n",
        "Updates parameters one data point at a time — faster but can be noisy.\n",
        "Code:\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "optimizer = SGD(learning_rate=0.01)\n",
        "\n",
        "Pros:\n",
        "Fast updates\n",
        "Works well with large datasets\n",
        "\n",
        "Cons:\n",
        "Fluctuates, may not converge smoothly\n",
        "\n",
        "3. Mini-Batch Gradient Descent\n",
        "Idea:\n",
        "A compromise — updates model using small batches of data at a time.\n",
        "\n",
        "Code:\n",
        "This is the default method in libraries like TensorFlow/Keras and PyTorch.\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=10)\n",
        "\n",
        "\n",
        "4. Momentum Optimizer\n",
        "Idea:\n",
        "Adds momentum to SGD — keeps moving in the same direction to speed up learning and avoid getting stuck.\n",
        "Code:\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
        "\n",
        "5. RMSprop\n",
        "Idea:\n",
        "Adjusts learning rate individually for each parameter using a moving average of squared gradients.\n",
        "\n",
        "Code:\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "optimizer = RMSprop(learning_rate=0.001)\n",
        "\n",
        "Best for:\n",
        "Recurrent Neural Networks (RNNs)\n",
        "Noisy, non-stationary objectives\n",
        "\n",
        "6. Adam (Adaptive Moment Estimation)\n",
        "Idea:\n",
        "Combines momentum + RMSprop — adjusts learning rate and direction using both mean and variance of gradients.\n",
        "\n",
        "Code:\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "Pros:\n",
        "Most commonly used optimizer\n",
        "Works well in most problems\n",
        "\n",
        "7. Adagrad / Adadelta / Nadam / FTRL (less common)\n",
        "These are variants that adapt learning rates in different ways or combine other strategies.\n",
        "\n",
        "Summary Table:\n",
        "\n",
        "| Optimizer | Key Feature                  | Best For                       |\n",
        "| --------- | ---------------------------- | ------------------------------ |\n",
        "| SGD       | Simple, one sample at a time | Basic models, large data       |\n",
        "| Momentum  | Adds speed                   | Faster convergence             |\n",
        "| RMSprop   | Scales learning rates        | RNNs, noisy data               |\n",
        "| Adam      | Momentum + RMSprop           | General-purpose, deep learning |\n",
        "| Adagrad   | Adapts to sparse data        | NLP, sparse data               |\n",
        "\n",
        "\n",
        "Q17 - What is sklearn.linear_model ?\n",
        "sklearn.linear_model is a module in the scikit-learn library that contains linear models for regression and classification tasks.\n",
        "\n",
        "These models assume a linear relationship between the input features and the target variable — either predicting a continuous value (regression) or class labels (classification).\n",
        "\n",
        "Common Models in sklearn.linear_model:\n",
        "1. LinearRegression\n",
        "For predicting continuous values (regression).\n",
        "\n",
        "Fits a line (or hyperplane) that minimizes the squared error between predicted and actual values.\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "\n",
        "2. LogisticRegression\n",
        "For binary or multi-class classification.\n",
        "Despite the name, it’s a classification algorithm, not regression.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "\n",
        "3. Ridge Regression\n",
        "Linear regression with L2 regularization (adds penalty for large weights).\n",
        "Helps prevent overfitting.\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "model = Ridge(alpha=1.0)\n",
        "\n",
        "4. Lasso Regression\n",
        "Linear regression with L1 regularization (encourages sparsity).\n",
        "Can eliminate irrelevant features (feature selection).\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "model = Lasso(alpha=0.1)\n",
        "\n",
        "\n",
        "5. ElasticNet\n",
        "Combines L1 and L2 penalties (Lasso + Ridge).\n",
        "\n",
        "from sklearn.linear_model import ElasticNet\n",
        "model = ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
        "\n",
        "6. SGDClassifier / SGDRegressor\n",
        "Uses Stochastic Gradient Descent to fit linear models efficiently, especially with large datasets.\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "model = SGDClassifier()\n",
        "\n",
        "Summary Table:\n",
        "| Model                | Task           | Regularization   | UseCase                             |\n",
        "| -------------------- | -------------- | ---------------- | ------------------------------------ |\n",
        "| `LinearRegression`   | Regression     | None             | Predicting continuous values         |\n",
        "| `LogisticRegression` | Classification | Optional (L1/L2) | Binary or multi-class classification |\n",
        "| `Ridge`              | Regression     | L2               | Prevents overfitting                 |\n",
        "| `Lasso`              | Regression     | L1               | Feature selection + regularization   |\n",
        "| `ElasticNet`         | Regression     | L1 + L2          | Mixed regularization                 |\n",
        "| `SGDClassifier`      | Classification | L1/L2            | Large-scale learning problems        |\n",
        "\n",
        "Q18- What does model.fit() do? What arguments must be given?\n",
        "Ans- The .fit() method is used to train a machine learning model. It tells the model to learn patterns in the data by finding the best parameters that minimize the loss function.\n",
        "\n",
        "In simple terms:\n",
        "model.fit() = “Learn from the data”\n",
        "\n",
        "What Happens Internally?\n",
        "When you call model.fit(X, y):\n",
        "The model takes input data (X) and target labels (y)\n",
        "It calculates predictions based on current parameters (initially random or zero)\n",
        "It computes the loss (error between predicted and actual values)\n",
        "It updates the model's internal parameters to reduce the error (e.g., using gradient descent)\n",
        "\n",
        "This process repeats for multiple iterations (epochs)\n",
        "\n",
        "Required Arguments:\n",
        "X — Input Features\n",
        "Type: Usually a NumPy array or Pandas DataFrame\n",
        "\n",
        "Shape: (n_samples, n_features)\n",
        "\n",
        "y — Target Values\n",
        "Type: Array, Series, or list\n",
        "\n",
        "Shape: (n_samples,) for 1D target (like regression or binary classification)\n",
        "\n",
        "Example 1: Linear Regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "Example 2: Keras Neural Network\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "Here, additional arguments:\n",
        "\n",
        "epochs: Number of times the model sees the entire training dataset\n",
        "batch_size: Number of samples per gradient update\n",
        "validation_split: Fraction of data to use for validation\n",
        "\n",
        "Summary:\n",
        "| Argument           | Meaning                                   | Required |\n",
        "| ------------------ | ----------------------------------------- | -------- |\n",
        "| `X`                | Input features                            | ✅ Yes    |\n",
        "| `y`                | Target values                             | ✅ Yes    |\n",
        "| `epochs`           | Number of training cycles (deep learning) | Optional |\n",
        "| `batch_size`       | Number of samples per update              | Optional |\n",
        "| `validation_split` | Holdout data for validation               | Optional |\n",
        "\n",
        "\n",
        "Q19- What does model.predict() do? What arguments must be given?\n",
        "Ans- The .predict() method is used to make predictions using a trained machine learning model.\n",
        "\n",
        "In simple terms:\n",
        "model.predict(X) = “Use what the model has learned to make predictions on new data”\n",
        "\n",
        "What Happens Internally?\n",
        "When you call model.predict(X):\n",
        "\n",
        "The model takes the input features X\n",
        "It applies the learned parameters (from .fit())\n",
        "It returns predicted values (either continuous or class labels)\n",
        "\n",
        "Required Argument:\n",
        "| Argument | Description                  | Required |\n",
        "| -------- | ---------------------------- | -------- |\n",
        "| `X`      | Input features to predict on | ✅ Yes    |\n",
        "\n",
        "X should be in the same format and shape as the input used during training (e.g., a 2D array or DataFrame).\n",
        "\n",
        "Example 1: Scikit-learn (e.g., Linear Regression)\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test data\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "Returns predicted continuous values.\n",
        "\n",
        "Example 2: Scikit-learn (e.g., Logistic Regression)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict class labels\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "Returns class labels (e.g., 0 or 1)\n",
        "\n",
        "Important Notes:\n",
        "You must train the model with .fit() before using .predict().\n",
        "\n",
        "Input X for .predict() must match the feature structure the model was trained on (same columns, order, and scaling if applied).\n",
        "\n",
        "Related Methods:\n",
        "predict_proba(X): For classification models, gives probabilities for each class.\n",
        "\n",
        "predict_log_proba(X): Returns log-probabilities (less common).\n",
        "\n",
        "\n",
        "Q20- What are continuous and categorical variables?\n",
        "Ans- In machine learning and statistics, variables (features) are classified into different types. The two most common types are:\n",
        "\n",
        "1. Continuous Variables\n",
        "Definition:\n",
        "Variables that can take any numerical value within a range, including decimals.\n",
        "\n",
        "Key Features:\n",
        "Measurable\n",
        "Have infinite possible values\n",
        "Support arithmetic operations (e.g., mean, standard deviation)\n",
        "\n",
        "Examples:\n",
        "Height (e.g., 170.5 cm)\n",
        "Weight (e.g., 65.2 kg)\n",
        "Temperature (e.g., 36.6°C)\n",
        "Income (e.g., $52,500.75)\n",
        "\n",
        "2. Categorical Variables\n",
        "Definition:\n",
        "Variables that represent groups or categories and contain a finite number of values.\n",
        "\n",
        "Key Features:\n",
        "\n",
        "Represent types or labels\n",
        "Can be nominal (no order) or ordinal (ordered)\n",
        "Usually require encoding for ML models\n",
        "\n",
        "Examples:\n",
        "Nominal (No inherent order):\n",
        "Color: Red, Blue, Green\n",
        "Gender: Male, Female\n",
        "Country: USA, Canada, India\n",
        "\n",
        "Ordinal (Ordered categories):\n",
        "Education level: High School < Bachelor’s < Master’s < PhD\n",
        "\n",
        "Satisfaction: Low, Medium, High\n",
        "\n",
        "Summary Table:\n",
        "\n",
        "| Feature Type | Data Type  | Example Values        | Mathematical Meaning | Needs Encoding |\n",
        "| ------------ | ---------- | --------------------- | -------------------- | -------------- |\n",
        "| Continuous   | Numeric    | 5.2, 88.7, 102.0      | Yes                  | No             |\n",
        "| Categorical  | Label/Text | \"Red\", \"Male\", \"High\" | No                   | Yes            |\n",
        "\n",
        "\n",
        "In Machine Learning:\n",
        "Continuous variables can go directly into most models.\n",
        "Categorical variables must often be converted using:\n",
        "Label Encoding\n",
        "One-Hot Encoding\n",
        "\n",
        "Q21- What is feature scaling? How does it help in Machine Learning?\n",
        "Ans- Feature scaling is the process of normalizing or standardizing the range of independent variables (features) in your dataset so that they all contribute equally to the model.\n",
        "\n",
        "Why Is Feature Scaling Important?\n",
        "Many machine learning algorithms assume or perform better when features are on similar scales.\n",
        "Without scaling, features with larger numeric ranges can dominate the learning process.\n",
        "\n",
        "Helps In:\n",
        "| Algorithm Type                                           | Needs Scaling? | Why?                                              |\n",
        "| -------------------------------------------------------- | -------------- | ------------------------------------------------- |\n",
        "| Gradient-based models (e.g., SGD, neural networks)       | ✅ Yes          | Gradient updates can be unstable if scales differ |\n",
        "| Distance-based models (e.g., KNN, K-Means, SVM)          | ✅ Yes          | Distance calculations become biased               |\n",
        "| Tree-based models (e.g., Decision Trees, Random Forests) | ❌ No           | Not sensitive to scale                            |\n",
        "\n",
        "\n",
        "🔧 Common Feature Scaling Techniques:\n",
        "✅ 1. Standardization (Z-score Normalization)\n",
        "Centers the data around 0\n",
        "\n",
        "Scales by standard deviation\n",
        "\n",
        "𝑧=(𝑥−mean)\n",
        "std\n",
        "\n",
        "z= std(x−mean)\n",
        "​\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "✅ 2. Min-Max Scaling (Normalization)\n",
        "Rescales features to a fixed range, usually [0, 1]\n",
        "\n",
        "𝑥′=(𝑥−min)\n",
        "(max−min)x ′ = (max−min)(x−min)\n",
        "​\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "✅ 3. Robust Scaling\n",
        "Uses median and IQR (interquartile range) to scale — resistant to outliers.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "💡 Example:\n",
        "Without scaling, a feature like \"income\" in thousands might overshadow \"age\" or \"years of experience\", leading to biased learning.\n",
        "\n",
        "✅ Summary:\n",
        "Method\tGood For\tHandles Outliers?\n",
        "StandardScaler\tNormal distributions\t❌\n",
        "MinMaxScaler\tBounded feature ranges\t❌\n",
        "RobustScaler\tData with outliers\t✅\n",
        "\n",
        "Q22- How do we perform scaling in Python?\n",
        "Ans- How to Perform Feature Scaling in Python (with scikit-learn)\n",
        "You can easily scale features using scikit-learn's preprocessing module, which provides several ready-to-use scalers.\n",
        "\n",
        "1. Standard Scaling (Z-score normalization)\n",
        "Scales data to have mean = 0 and standard deviation = 1.\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "2. Min-Max Scaling (Normalization)\n",
        "Scales data to a fixed range, usually between 0 and 1.\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "3. Robust Scaling\n",
        "Scales data using median and IQR (interquartile range). Good for handling outliers.\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "4. Using ColumnTransformer for Selective Scaling\n",
        "To apply scaling only to numerical columns in a dataset with mixed types (e.g., categorical + numerical):\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), ['age', 'income']),\n",
        "        ('cat', OneHotEncoder(), ['gender', 'city'])\n",
        "    ]\n",
        ")\n",
        "\n",
        "X_processed = preprocessor.fit_transform(data)\n",
        "Notes:\n",
        "fit_transform() is used on training data\n",
        "\n",
        "For test/validation data, use transform() only to avoid data leakage\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "Example:\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Sample data\n",
        "df = pd.DataFrame({\n",
        "    'age': [25, 32, 47, 51],\n",
        "    'salary': [40000, 54000, 75000, 88000]\n",
        "})\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled = scaler.fit_transform(df)\n",
        "scaled_df = pd.DataFrame(scaled, columns=df.columns)\n",
        "\n",
        "print(scaled_df)\n",
        "\n",
        "Q23- What is sklearn.preprocessing?\n",
        "Ans- What is sklearn.preprocessing?\n",
        "sklearn.preprocessing is a module in scikit-learn that provides a wide range of tools for data preprocessing — the steps taken to clean, transform, and prepare your data before feeding it into a machine learning model.\n",
        "\n",
        "Preprocessing helps improve model performance, accuracy, and training speed.\n",
        "\n",
        "What Can You Do with sklearn.preprocessing?\n",
        "Here are the most commonly used features:\n",
        "\n",
        "1. Feature Scaling\n",
        "StandardScaler\n",
        "Standardizes features to mean = 0 and std = 1\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "MinMaxScaler\n",
        "Scales features to a fixed range (usually 0 to 1)\n",
        "\n",
        "RobustScaler\n",
        "Uses median and IQR, handles outliers better\n",
        "\n",
        "2. Encoding Categorical Variables\n",
        "LabelEncoder\n",
        "Converts categories to numbers (e.g., \"red\", \"blue\" → 0, 1)\n",
        "\n",
        "OneHotEncoder\n",
        "Converts categories to binary vectors\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "X_encoded = encoder.fit_transform(X)\n",
        "\n",
        "3. Handling Missing or Sparse Data\n",
        "Imputer (now SimpleImputer in sklearn.impute)\n",
        "Fills missing values using mean, median, or constant\n",
        "\n",
        "4. Generating Polynomial Features\n",
        "PolynomialFeatures\n",
        "Expands features into polynomial combinations\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "5. Binarization and Normalization\n",
        "Binarizer\n",
        "Converts values above/below a threshold to 1/0\n",
        "\n",
        "Normalizer\n",
        "Scales rows to have unit norm (useful for text data and clustering)\n",
        "\n",
        "Q24- How do we split data for model fitting (training and testing) in Python?\n",
        "Ans-  How to Split Data for Training and Testing in Python\n",
        "In machine learning, splitting your dataset into training and testing sets is essential to:\n",
        "Train your model on one part (training set)\n",
        "Evaluate its performance on unseen data (test set)\n",
        "\n",
        "Use train_test_split() from scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Example:\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X = features, y = target variable\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,              # Input and output\n",
        "    test_size=0.2,     # 20% for testing, 80% for training\n",
        "    random_state=42    # Ensures reproducibility\n",
        ")\n",
        "\n",
        "Parameters:\n",
        "Parameter\tPurpose\n",
        "X, y\tYour input features and target variable\n",
        "test_size\tFraction or count of data for testing (e.g., 0.2 = 20%)\n",
        "train_size\tOptional – set training portion explicitly\n",
        "random_state\tFixes the random split so results are repeatable\n",
        "shuffle\tWhether to shuffle data before splitting (default is True)\n",
        "stratify\tKeeps class distribution consistent (important for classification)\n",
        "\n",
        "Example with Stratification (classification):\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.25,\n",
        "    stratify=y,\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "Summary:\n",
        "Use train_test_split() to divide your data\n",
        "Adjust test_size and random_state as needed\n",
        "Use stratify=y for imbalanced classification problems\n",
        "\n",
        "Q25- Explain data encoding?\n",
        "Ans- Data encoding is the process of converting categorical (non-numeric) data into numerical values so that machine learning models can understand and use them.\n",
        "\n",
        "Most ML algorithms (like linear regression, decision trees, neural networks) work only with numbers, not text labels.\n",
        "\n",
        "Why Do We Need Encoding?\n",
        "Models can’t handle strings like \"Male\" or \"Red\" directly\n",
        "Encoding allows us to represent categories as numbers\n",
        "Ensures logical representation of non-numeric data\n",
        "\n",
        "Common Data Encoding Techniques:\n",
        "1. Label Encoding\n",
        "Converts each category into a unique integer.\n",
        "\n",
        "Good for ordinal (ordered) categorical data.\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['gender_encoded'] = le.fit_transform(df['gender'])  # 'Male' → 1, 'Female' → 0\n",
        "Warning: For nominal data (unordered), this may wrongly imply order or hierarchy.\n",
        "\n",
        "2. One-Hot Encoding\n",
        "Converts categories into binary columns (0 or 1)\n",
        "Best for nominal (unordered) data\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoded = encoder.fit_transform(df[['city']])\n",
        "\n",
        "# Convert to DataFrame for readability\n",
        "encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(['city']))\n",
        "\"City\" column with values ['Paris', 'London', 'Tokyo'] becomes:\n",
        "\n",
        "csharp\n",
        "[Paris=1, London=0, Tokyo=0]\n",
        "[Paris=0, London=1, Tokyo=0]\n",
        "...\n",
        "\n",
        "3. Ordinal Encoding\n",
        "Converts categories into integers based on defined order\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "education_levels = [['High School'], ['Bachelor'], ['Master'], ['PhD']]\n",
        "encoder = OrdinalEncoder(categories=[['High School', 'Bachelor', 'Master', 'PhD']])\n",
        "encoder.fit_transform(education_levels)\n",
        "\n",
        "4. Binary Encoding, Hashing, and Target Encoding\n",
        "(Advanced methods used in high-cardinality situations — e.g., many unique categories)\n",
        "\n",
        "Summary Table:\n",
        "| Encoding Type    | Best For         | Example Use Case         |\n",
        "| ---------------- | ---------------- | ------------------------ |\n",
        "| Label Encoding   | Ordinal          | Education level, ranking |\n",
        "| One-Hot Encoding | Nominal          | Gender, city, color      |\n",
        "| Ordinal Encoding | Ordinal          | Satisfaction levels      |\n",
        "| Binary/Hashing   | High cardinality | ZIP codes, usernames     |\n"
      ],
      "metadata": {
        "id": "B9Ti-UrU03sc"
      }
    }
  ]
}